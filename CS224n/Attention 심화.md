# Attention 심화

- Attention : seq2seq을 크게 개선하는 기법, seq2seq에 기반
- seq2seq 모델이 마치 인간처럼 필요한 정보에만 주목(Attention)하게 해줌 → 그래서 어텐션

- 우선 seq2seq에서 인코더는 source sentence를 분석, 디코더는 그 값을 바탕으로 target sentence로 계산
- seq2seq 모델의 한계점
    - 인코더에서 입력문장의 길이가 얼마나 길든 상관없이 마지막 단계에서 인코딩을 진행하기 때문에 항상 같은 길이의 벡터로 전환함 
    → 만약 엄청나게 긴 길이의 문장이 입력된다면 그 긴 문장에 대한 정보가 잘 담길리 없음

- 이러한 문제점을 해결하기 위해 여러 개선책을 내다보니 Attention 기법 탄생

1. 인코더 개선
- 먼저 문장의 길이를 반영하기 위해 encoder의 각 계층들이 내뱉는 hidden state를 따로 정리했고, 이를 이용해 인코더 벡터 구성
- (ex. 단어 5개의 문장 → 5개의 hidden state 각각의 정보 입력)

1. 디코더 개선
- 어떤것에 주목하도록 디코더 개선
- 보통 사람이 번역을 할 때,
- A라는 단어를 번역하려면 그 A 단어와 대응되는 여러 단어들을 계속 찾아보면서 이중 B라는 단어가 가장 깊게 대응된다고 판단한 후, A → B단어로 번역할 것임
- 즉 A단어를 번역하기 위해 B 단어에 특별히 Attention하여 변환

1. 가중치 계산
- 인코더의 각 단계별로 나온 벡터와 디코더의 각 단계별로 나온 벡터간의 내적을 통해 가중치를 계산
- 이것이 바로 Attention score

- 이처럼 입력과 출력의 각 단어들 간에 서로 어떤 단어들끼리 연관되어있는가 라는 대응관계를 seq2seq에 학습시키자는 것이 어텐션 매커니즘의 핵심