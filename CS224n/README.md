- 머신러닝 학습을 이제 막 시작한 시절, 딥러닝 개념도 모른채로 무턱대로 수강
- 초반엔 다소 어려웠으나, 해당 강의에서 딥러닝 개념도 처음부터 짚고 넘어가기 때문에 학습에 지장은 없었음

- Word2Vec, GloVe, NER, Contingency / Dependency Parsing, Language Modeling, N-Gram, RNN, Vanishing Gradient, Exploding Gradient, LSTM, Seq2Seq, Attention, Transformer, GRU, QA 등 NLP에 필요한 수많은 기본 개념을 학습 

- md 파일을 하나씩 올리고 싶었으나, 사진 문제로 노션 링크로 첨부

https://cherry-orbit-69a.notion.site/CS224n-8d8132f7df2844878ef39b84e07728e1

----
- **Lecture 1** : NLP 개요, Wordnet, Word2Vec, Optimization
- **Lecture 2** : Word2Vec, Optimization, SGD, Negative Sampling, GloVe, Word Embedding, Word Ambiguity
- **Lecture 3** : NER, Binary Window Classification, Forward Propagation, BackPropagation
- **Lecture 4** : Consistency / Depoendency Parsing
- **Lecture 5** : Neural Classification, Drop out, Non-Linearities, Initialization, Optimizer, Language Model, N-Gram, Neural Language Model, RNN 
- **Lecture 6** : RNN, Vanishing/Exploding Gradient, LSTM, Bidirectional RNN
- **Lecture 7 & 8** : Machine Translation, Conditional Probability, SMT, NMT, Greedy decoding, Beem search decoding, Attention
- **Attention 심화** : 보다 더 구체적인 Attention과 관련된 내용
- **Lecutre 9** : Issues with RNN, Self-Attention, Transformer, 
- **Lecture 10** : Subword Model, Pretraining, BERT, GPT-3
- **Lecture 11** : Question Answering, Reading Comprehension, BiDAF, Retriver-Reader Framework
- **Lecture 12** : NLG
- **Lecture 13** : Conference Resolution, Coreference
- **Lecture 14** : T5
- **Lecture 15** : Techniques to add knowledge to Language Model, KGLM
- **Lecture 16** : Ethics about NLP
- **Lecture 17** : Model Evaluation
