# [CS224n] Lecture 14

![1.PNG](%5BCS224n%5D%20Lecture%2014%20b8ff5a994d3b4c3b8234c085c89d05c9/1.png)

- pretraining - 비지도학습
- fine tuning - 지도학습

# Text-to-Text Transfer Transformer(T5)

![2.PNG](%5BCS224n%5D%20Lecture%2014%20b8ff5a994d3b4c3b8234c085c89d05c9/2.png)

![3.PNG](%5BCS224n%5D%20Lecture%2014%20b8ff5a994d3b4c3b8234c085c89d05c9/3.png)

- Text as input, Text as output
- 번역, 텍스트 분류, 문장간유사도 측정, 요약 등 모든 NLP task를 통합

## Unlabeld text 훈련 방법

![4.PNG](%5BCS224n%5D%20Lecture%2014%20b8ff5a994d3b4c3b8234c085c89d05c9/4.png)

- 몇 개의 토큰을 랜덤으로 고름
- 빈칸을 뚫어놓은 input을 주며, 빈칸을 채우는 것을 목적으로 훈련

# T5의 구조

![5.PNG](%5BCS224n%5D%20Lecture%2014%20b8ff5a994d3b4c3b8234c085c89d05c9/5.png)

- Pretrain
    - BERT와 동일한 크기의 인코더 디코더 transformer
    - T5 모델에서 새로이 제안한 C4 dataset 활용해서 훈련

- BERT은 텍스트의 분류, span prediction 등을 위주로 활용되지만, T5는 모든 NLP 기법에 활용가능
- Encoder - Decoder, Language model, Prefix Language model 중에선 Encoder-Decoder 모델이 가장 성능이 좋았음
- 결론적으로 Encoder-decoder 구조에, span prediction objective에 대해, C4 dataset으로 Multi-task pre-training으로 훈련을 한 뒤 더 큰 모델과 긴 시간동안 훈련을 하며 T5 모델 구축

## Multilingual model

![6.PNG](%5BCS224n%5D%20Lecture%2014%20b8ff5a994d3b4c3b8234c085c89d05c9/6.png)

- T5에서 여러 언어를 통합

## How much knowledge does a language model pick up during pre-training?

![7.PNG](%5BCS224n%5D%20Lecture%2014%20b8ff5a994d3b4c3b8234c085c89d05c9/7.png)

- Reading Comprehension : 주어진 문맥에서 나온 질문에 응답

![8.PNG](%5BCS224n%5D%20Lecture%2014%20b8ff5a994d3b4c3b8234c085c89d05c9/8.png)

- Open-domain Question Answering : 수많은 데이터베이스 속에서 찾아서 질문 응답

![9.PNG](%5BCS224n%5D%20Lecture%2014%20b8ff5a994d3b4c3b8234c085c89d05c9/9.png)

- Closed-Book Question Answering : pre-training때에 학습한 데이터를 바탕으로 응답
    - T5의 경우 사전학습을 대량의 데이터로 진행하기 때문에, Closed-Book 가능

## Do large language models memorize their training data?

![10.PNG](%5BCS224n%5D%20Lecture%2014%20b8ff5a994d3b4c3b8234c085c89d05c9/10.png)

- Language model이 학습을 잘해서 정보를 이해하고 있는 것인지, 아니면 단순히 외우기만 하는것인지 여부
- 만약 단순 암기라면 GPT-2 같이 대량의 web 기반 데이터로 학습할 경우, 여러가지 문제 발생
    - 개인정보가 training dataset에 포함되어있고, 제대로 block되지 않을 경우 Large LM Decoder가 generation 하는 정보가 개인정보가 될 수 있음 ⇒ 윤리적 문제!

- 이러한 문제점을 해결하기 위해 학습 및 평가하는 방법

![11.PNG](%5BCS224n%5D%20Lecture%2014%20b8ff5a994d3b4c3b8234c085c89d05c9/11.png)

## Can we close the gap between large and small models by improving the Trnasformer architecture?

![12.PNG](%5BCS224n%5D%20Lecture%2014%20b8ff5a994d3b4c3b8234c085c89d05c9/12.png)

기존의 T5 구조

- Factorized embedding matrix
- share the embedding matrix and the softmax output layer
- Mixture of softmaxes, adaptive softmax
- Different ways of normalizing, initializing the model
- different attention mechanism
- alternatives to attention mechanism 등등의 방법을 통해 개선시도