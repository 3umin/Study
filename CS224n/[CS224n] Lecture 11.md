# [CS224n] Lecture 11

# Question Answering

![1](https://user-images.githubusercontent.com/99728502/226582238-ee90c49e-df23-4f7e-9e5d-58114b340d50.png)

- ì‚¬ëŒì˜ ì§ˆë¬¸ì— ë§ê²Œ ìì—°ì–´ë¡œ ìë™ì ìœ¼ë¡œ ì‘ë‹µí•˜ëŠ” ì‹œìŠ¤í…œ
- ì ì ˆí•œ ë¬¸ì„œë¥¼ ì¶œë ¥í•˜ëŠ” ì •ë³´ ê²€ìƒ‰(Information Retriever)ê³¼ ë‹¤ë¥´ê²Œ, ê°„ë‹¨í•œ êµ¬ë¡œ ì´ë£¨ì–´ì§„ ì •ë‹µì„ ì¶œë ¥
- ë¶„ë¥˜ê°€ í•µì‹¬
    - ì–´ë– í•œ ì†ŒìŠ¤ë¡œë¶€í„° ì •ë³´ë¥¼ í™œìš©í•˜ëŠ”ê°€?
        - ì±…ì˜ êµ¬ì ˆ, ì›¹ ë¬¸ì„œ, ì‚¬ì§„, knowledge bases, tables ë“±ë“±ë“±
    - ì§ˆë¬¸ì˜ ìœ í˜•
        - ê°„ë‹¨í•œ vs ë³µì¡í•œ, closed-domain(íŠ¹ì •ì˜ì—­ í•œì •) vs open-domain ë“±ë“±
    - ì‘ë‹µì˜ ìœ í˜•
        - ì§§ì€ í…ìŠ¤íŠ¸ ë¬¸êµ¬, ë‹¨ë½, ë¦¬ìŠ¤íŠ¸, O X ë“±ë“±

- ê°€ì¥ í”í•œ ì˜ˆì‹œ

![2](https://user-images.githubusercontent.com/99728502/226582253-a898864e-854a-48a8-8e3c-97be7bc84739.png)

- QAì˜ êµ¬ì¡°
    - ëŒ€ë¶€ë¶„ì˜ QAëŠ” end-to-end trainingê³¼ BERTê°™ì€ pretraining ê¸°ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë£¨ì–´ì§

![3](https://user-images.githubusercontent.com/99728502/226582267-ba1acb06-4740-4743-897d-a3894850aeac.png)

- ìš”ì¦˜ì—ëŠ” ë¬¸ìì ì¸ QAë¥¼ ë„˜ì–´ì„œ, ì´ë¯¸ì§€ë‚˜ ì˜ìƒ ë“±ì—ì„œì˜ ì§ˆë¬¸ê¹Œì§€ ë‹µí•˜ëŠ” ì‹œìŠ¤í…œìœ¼ë¡œ ë°œì „
    
    ![4](https://user-images.githubusercontent.com/99728502/226582283-8db77cb1-4431-4ca1-a2e3-cfba471a8cf4.png)


# Reading comprehension

![5](https://user-images.githubusercontent.com/99728502/226582302-4094d0cc-97b5-4232-b6ef-eda0fea329db.png)

- ì •í•´ì§„ êµ¬ì ˆì„ ì´í•´í•œ ë’¤ ê·¸ ë‚´ìš©ì— ê´€ë ¨ëœ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ê²ƒ
- â€˜ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥â€™ì´ ê°€ì¥ ê°•ë ¥í•œ ì´í•´ì˜ í‘œì‹œì´ë¯€ë¡œ, reading comprehensionì€ ë§¤ìš° ì¤‘ìš”
- ê¸´ ë¬¸ì¥ì—ì„œ ì •ë³´ì˜ ì¶”ì¶œ, ê·¸ë¦¬ê³  ì˜ë¯¸ì ì¸ ì—­í• ì˜ ë¼ë²¨ë§ì´ ê°€ëŠ¥

![6](https://user-images.githubusercontent.com/99728502/226582326-e4589dd0-bc36-43f5-a19d-87ed8638ec8b.png)

# Standford Question Answering dataset(SQuAD)

- ì•½ 10ë§Œê°œì˜ ì£¼ì„ì´ ë‹¬ë¦° (êµ¬ì ˆ, ì§ˆë¬¸, ì •ë‹µ) 3ê°œ ì„¸íŠ¸
    - êµ¬ì ˆì€ ì˜ì–´ ìœ„í‚¤í˜ë””ì•„ì—ì„œë„ ìˆ˜ì§‘
    - ì§ˆë¬¸ì€ ì¼ë°˜ì¸ë“¤ì—ê²Œì„œ ìˆ˜ì§‘
    - ê° ì •ë‹µì€ êµ¬ì ˆ ì†ì— ìˆëŠ” ì§§ì€ ë¬¸êµ¬ë¡œ ì‘ì„±
    - ê° ì •ë‹µì€ 3ê°œì˜ Gold answerë¡œ ìˆ˜ì§‘(ë‹¤ì–‘í•œ ì •ë‹µì´ ìˆì„ ìˆ˜ ìˆê¸° ë•Œë¬¸)
- í‰ê°€ë°©ë²• : Exact match(ë§ìœ¼ë©´ 1, í‹€ë¦¬ë©´ 0)ê³¼ F1-score(ë¶€ë¶„ì ìˆ˜)ë¥¼ í•¨ê»˜ í™œìš©

# Neural models for reading comprehension

- Input : $C = (c_1,c_2, ...\ ,c_N), Q = (q_1, q_2, ... \ , q_M)$
- Output : $1 \le start \le end \le N$

- seq2seq model with attention

<img width="281" alt="7" src="https://user-images.githubusercontent.com/99728502/226582343-24775f48-b62f-407e-9df0-2af94901a602.png">

passageì— ìˆëŠ” ì–´ë–¤ ë‹¨ì–´ê°€ ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ì´ ìˆëŠ”ì§€ë¥¼ ì•Œì•„ì•¼í•¨

## BiDAF : Bidirectional Attetion Flow model

<img width="370" alt="8" src="https://user-images.githubusercontent.com/99728502/226582357-5d80eef9-0588-424c-ae2a-ba6e7131ccef.png">

### BiDAF: Encoding

<img width="608" alt="9" src="https://user-images.githubusercontent.com/99728502/226582372-24059fd4-4f0d-4bab-a62c-c48fc892f47b.png">

- ê°ê°ì˜ ë¬¸ë§¥ê³¼ ì¿¼ë¦¬ ì†ì˜ ë‹¨ì–´ì— ëŒ€í•´ word embedding(GloVe)ê³¼ character embeddingì„ ë°˜ë³µí•œ ìë£Œë¥¼ ì‚¬ìš©
    
    $e(c_i) = f([GloVe(c_i); charEmb(c_i]) \ \ \ \ \ e(qi) = f([GloVe(q_i);charEmb(q_i)])$
    
- ê·¸ëŸ° ë‹¤ìŒ, ë¬¸ë§¥ê³¼ ì¿¼ë¦¬ë¥¼ ìœ„í•œ contextual embeddingì„ ë§Œë“¤ê¸° ìœ„í•´ ë‘ê°œì˜ ì–‘ë°©í–¥ LSTMì„ ê°ê° ì‚¬ìš©

### BiDAF: Attention

<img width="398" alt="10" src="https://user-images.githubusercontent.com/99728502/226582382-672aad33-19fe-4cdb-af27-aaacd314532e.png">

- ë¨¼ì €, ëª¨ë“  ì§ $(c_i, q_j)$ì— ëŒ€í•´ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê³„ì‚°:
    
    $S_{i,j} = w^T_{sim}[c_i;q_j;c_i \odot q_j] \in R \ \ \ \ \ \ \ w_{sim} \in R^{6H}$
    
- context-to-query attention : ê°ê°ì˜ context ë‹¨ì–´ì— ëŒ€í•´, ê°€ì¥ ê´€ë ¨ëœ ë‹¨ì–´ë¥¼ query ë‹¨ì–´ì¤‘ì—ì„œ ë½‘ìŒ
    
    $\alpha_{i,j} = softmax_j(S_{i,j}) \in R \ \ \ \ \ \ a_i = \sum_{j=1}^M \alpha_{i,j}q_j \in R^{2H}$
    

<img width="244" alt="11" src="https://user-images.githubusercontent.com/99728502/226582401-818f1cc4-48d9-4a67-b529-c24daa6e57fe.png">

- query-to-context attention : ê°ê°ì˜ query ë‹¨ì–´ì— ëŒ€í•´ ê°€ì¥ ê´€ë ¨ìˆëŠ” context ë‹¨ì–´ë¥¼ í•˜ë‚˜ì”© ë½‘ìŒ
    
    $\beta_i = softmax_i(max_{j=1}^M(S_{i,j})) \in R^N \ \ \ \ \ \ \ b_i = \sum_{i=1}^N \beta_ic_i \in R^{2H}$
    
<img width="457" alt="12" src="https://user-images.githubusercontent.com/99728502/226582431-4a905493-06ae-426b-a1b4-0bb9635cb52f.png">

- ìµœì¢… ê²°ê³¼ëŠ” $g_i =[c_j;a_i;c_i \odot a_i;c_i \odot b_i] \in R^{8H}$
- Modeling Layer : context word ì‚¬ì´ì˜ ìƒí˜¸ì‘ìš©ì„ ëª¨ë¸ë§
    
    $m_i = BiLSTM(g_i) \in R^{2H}$
    
- Output layer : startì™€ end positionì„ ì˜ˆì¸¡í•˜ëŠ” ë‘ê°œì˜ ë¶„ë¥˜ê¸°
    
    $p_{start} = softmax(w^T_{start}[g_i;m_i]) \ \ \ \ \ \ \ \ \ p_{end} = softmax(w^T_{end}[g_i;m_i']) \\m'_i = BiLSTM(m_i)$
    
- ìµœì¢… ì†ì‹¤í•¨ìˆ˜ëŠ” $L = -log(p_{start}(s^*)) - log(p_{end}(e^*))$ë¡œ í‘œí˜„

## Bert for reading comprehension

<img width="393" alt="13" src="https://user-images.githubusercontent.com/99728502/226582457-11d262ca-e499-4755-9d09-b447c8e5f3be.png">

- BERTëŠ” ë§ì€ ì–‘ì˜ í…ìŠ¤íŠ¸ì— ëŒ€í•´ pre-trained ëœ deepí•œ ì–‘ë°©í–¥ Transformer encoder

<img width="226" alt="14" src="https://user-images.githubusercontent.com/99728502/226582477-83c616b7-d122-4a68-8068-a56cfd0241c5.png">

- Question = Segment A
- Passage(êµ¬ì ˆ) = Segment B
- Answer = Segment Bì—ì„œ ë‘ê°œì˜ endpointë¥¼ ì˜ˆì¸¡

- BERTì™€ BiDAFì˜ ë¹„êµ
    - BERT ëª¨ë¸ì€ 110 million or 330 million ì •ë„ì˜ ì•„ì£¼ ë§ì€ ëª¨ìˆ˜ê°€ ì¡´ì¬í•˜ëŠ” ë°˜ë©´, BiDAFëŠ” ~2.5million parameter
    - BiDAFëŠ” ì—¬ëŸ¬ê°œì˜ ì–‘ë°©í–¥ LSTMì„ í†µí•´ ë§Œë“¤ì–´ì§€ê³ , BERTëŠ” Transformerì„ í†µí•´ ë§Œë“¤ì–´ì§
    - BERTëŠ” pre-train ë˜ì—ˆì§€ë§Œ, BiDAFëŠ” GloVeë¥¼ í†µí•´ì„œë§Œ ë§Œë“¤ì–´ì§
    - BiDAFì™€ ë‹¤ë¥¸ ëª¨ë¸ì€ ì§ˆë¬¸ê³¼ êµ¬ì ˆì˜ ìƒí˜¸ì‘ìš©ì„ ëª¨ë¸ë§ í•˜ëŠ”ë°ì— ì¤‘ì 
    - BERTëŠ” ì§ˆë¬¸ê³¼ êµ¬ì ˆ ì‚¬ì´ì—ì„œ self-attentionì„ ì‚¬ìš© â†’ ë‘˜ì´ ê·¼ë³¸ì ìœ¼ë¡  ìœ ì‚¬

# Can we design better-pretraining objectives?

<img width="355" alt="15" src="https://user-images.githubusercontent.com/99728502/226582499-9e6ae03e-0906-46ad-a013-4c53ce9a7f10.png">

Two ides

- 15%ì˜ ë¬´ì‘ìœ„ ë‹¨ì–´ ëŒ€ì‹  ê·¼ì ‘í•œ ë‹¨ì–´ë“¤ì„ ë§ˆìŠ¤í‚¹
- ë§ˆìŠ¤í‚¹ëœ ë‹¨ì–´ì˜ ì˜ˆì¸¡ì„ í•  ë•Œ 2ê°œì˜ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©
    
    $y_i = f(x_{s - 1}, x_{e+1}, p_{i-s+1})$
    

SQuAD ë°ì´í„°ì…‹ì—ì„œ ì¸ê°„ì˜ ì ìˆ˜ë³´ë‹¤ ì´ë¯¸ ë” ë†’ì€ ì ìˆ˜ë¥¼ ê¸°ë¡í–ˆëŠ”ë°, ê·¸ë ‡ë‹¤ë©´ reading comprehensionì€ ì™„ë²½í•œê°€?

- No, ìƒë°˜ë˜ëŠ” ë¬¸êµ¬ì— ëŒ€í•œ reading comprehensionì˜ ì„±ëŠ¥ì€ ë‚®ìŒ
- ê·¸ë¦¬ê³  í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ì—ì„œ í›ˆë ¨í•œ ëª¨ë¸ì€ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì¼ë°˜í™”í•˜ì§€ ëª»í•¨

# Open-domain Question Answering

<img width="345" alt="16" src="https://user-images.githubusercontent.com/99728502/226582520-f4d62652-4033-433a-9afb-3b9b0fee1bc4.png">

- ì£¼ì–´ì§„ ì§ˆë¬¸ì— ê´€ë ¨ëœ êµ¬ì ˆì´ ë”°ë¡œ ì—†ê³ , ìœ„í‚¤í˜ë””ì•„ ê°™ì´ ë§ì€ ë¬¸ì„œë¥¼ ëª¨ì•„ë‘” ê³³ì— ìˆëŠ” êµ¬ì ˆì„ í™œìš©
- ë‹µì´ ì–´ë””ìˆëŠ”ì§€ëŠ” ëª¨ë¥´ê³ , í•˜ë‚˜ì˜ open-domain ì§ˆë¬¸ì— ëŒ€í•´ ë‹µì„ ì£¼ëŠ” ê²ƒì´ ëª©í‘œ
    
    â†’ ë”ìš± ë” ì–´ë µì§€ë§Œ ë³´ë‹¤ ë” í˜„ì‹¤ì ì¸ ë¬¸ì œ!
    

## Retriver-reader framework(ê²€ìƒ‰-ì½ê¸° í”„ë ˆì„ì›Œí¬)

- Retriver : $f(ğ’Ÿ, Q)$  â†’ $P_1, ... \ , P_K$,              ğ’Ÿ: large collection of documents
- Reader : $g(Q, \left\{P_1, ...\ , P_K \right\}) = A$              A : an answer string

<img width="412" alt="17" src="https://user-images.githubusercontent.com/99728502/226582530-4d72af19-e91f-4109-8687-6f400346e989.png">

- Retriever(ê²€ìƒ‰ê¸°)ë„ í›ˆë ¨í•  ìˆ˜ ìˆìŒ!
    - ê°ê°ì˜ êµ¬ì ˆì€ BERTë¥¼ ì‚¬ìš©í•´ì„œ vectorë¡œ ì¸ì½”ë”© ë  ìˆ˜ ìˆê³ , retriver scoreì€ ì§ˆë¬¸ì— ëŒ€í•œ ì¸ì½”ë”©ê°’ê³¼(ë²¡í„°) êµ¬ì ˆì— ëŒ€í•œ ì¸ì½”ë”© ê°’ì˜ ë‚´ì ì„ í†µí•´ ê³„ì‚°í•  ìˆ˜ ìˆìŒ
    - í•˜ì§€ë§Œ, ë„ˆë¬´ë‚˜ë„ ë§ì€ êµ¬ì ˆì´ ìˆê¸° ë•Œë¬¸ì— ëª¨ë“  ë²¡í„°ë¥¼ êµ¬í•˜ëŠ”ë°ì— ìˆì–´ ì–´ë ¤ì›€ì´ ìˆìŒ

- Reader ëª¨ë¸ì´ ì—†ì–´ë„ ë˜ëŠ” ê°€ëŠ¥ì„±ì´ ì¡´ì¬
    - ëª¨ë“  êµ¬ë¬¸ì„ ì¸ì½”ë”©í•˜ëŠ”ê²Œ ê°€ëŠ¥, dense vectorì„ ì´ìš©í•˜ì—¬ BERTë¥¼ ì“°ì§€ ì•Šê³ , nearest neighbor searchë¥¼ í•  ìˆ˜ ìˆìŒ
